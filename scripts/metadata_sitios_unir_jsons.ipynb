{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2faed5c1-4a75-4180-a28a-dd5269bad456",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: azure-storage-blob in /local_disk0/.ephemeral_nfs/envs/pythonEnv-54a61514-7b2b-433d-b3d0-9afb37f56cdf/lib/python3.9/site-packages (12.17.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-54a61514-7b2b-433d-b3d0-9afb37f56cdf/lib/python3.9/site-packages (from azure-storage-blob) (4.7.1)\nRequirement already satisfied: isodate>=0.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-54a61514-7b2b-433d-b3d0-9afb37f56cdf/lib/python3.9/site-packages (from azure-storage-blob) (0.6.1)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.9/site-packages (from azure-storage-blob) (3.4.8)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-54a61514-7b2b-433d-b3d0-9afb37f56cdf/lib/python3.9/site-packages (from azure-storage-blob) (1.29.3)\nRequirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\nRequirement already satisfied: requests>=2.18.4 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.27.1)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.0)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2021.10.8)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987b8676-c62c-484a-b8f2-b94bd7289cab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre del último archivo: metadata_sitios/8.json\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# Codigo que funciona correctamente\n",
    "##################################################################\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import pandas as pd\n",
    "import io\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "#spark = pyspark.sql.SparkSession.builder.appName(\"Connect to MySQL\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"EjemploDatabricks\").getOrCreate()\n",
    "\n",
    "# Información de tu cuenta de almacenamiento y el folder\n",
    "storage_account_name = \"datalakeproyectoreviews\"\n",
    "storage_account_access_key = \"nUJSljEWbc2ePVYZkrbcBkXChgXK4QMHTS5vVuCnyjZIxQSbxTKzJmfQqMq2c5G92zGOrNeFzA+w+AStjnDikg==\"\n",
    "container_name = \"datalakereviews\"\n",
    "folder_path = \"metadata_sitios/\"\n",
    "\n",
    "# Conecta al servicio de Blob Storage\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_access_key};EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Obtén una referencia al contenedor\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# Lista los blobs en el folder\n",
    "blobs = container_client.list_blobs(name_starts_with=folder_path)\n",
    "\n",
    "# Obtiene el nombre del último archivo (blob)\n",
    "last_blob_name = None\n",
    "last_modified = None\n",
    "\n",
    "for blob in blobs:\n",
    "    if last_modified is None or blob['last_modified'] > last_modified:\n",
    "        last_blob_name = blob['name']\n",
    "        last_modified = blob['last_modified']\n",
    "\n",
    "# Elimina la extensión duplicada \".json\" si existe\n",
    "if last_blob_name is not None:\n",
    "    last_blob_name = last_blob_name.replace(\".json.json\", \".json\")\n",
    "\n",
    "if last_blob_name is not None:\n",
    "    print(\"Nombre del último archivo:\", last_blob_name)\n",
    "    \n",
    "    # Descarga el contenido del archivo desde Azure Blob Storage\n",
    "    blob_client = container_client.get_blob_client(blob=last_blob_name)\n",
    "    json_data = blob_client.download_blob()\n",
    "    json_text = json_data.readall()\n",
    "\n",
    "    # Carga el contenido JSON en un DataFrame de Pandas\n",
    "    df = pd.read_json(io.BytesIO(json_text), lines=True)\n",
    "\n",
    "    \n",
    "    # Ahora puedes trabajar con df como un DataFrame de Pandas\n",
    "    # print(df.head())  # Esto imprimirá las primeras filas del DataFrame\n",
    "else:\n",
    "    raise Exception(\"No se encontraron archivos en la carpeta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44793fed-00ea-4bcf-9edd-c86bd8c6b4d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# en cada options levanto una tabla\n",
    "con_business = {\n",
    "  \"host\": \"reviewsdatabase.mysql.database.azure.com\",\n",
    "  \"port\": 3306,\n",
    "  \"database\": \"reviews\",\n",
    "  \"dbtable\": \"business\",\n",
    "  \"user\": \"reviewsadmin\",\n",
    "  \"password\": \"Our_password@2023\",\n",
    "  \"require_secure_transport\": 'On',\n",
    "  \"useSSL\": True\n",
    "}\n",
    "\n",
    "con_catego = {\n",
    "  \"host\": \"reviewsdatabase.mysql.database.azure.com\",\n",
    "  \"port\": 3306,\n",
    "  \"database\": \"reviews\",\n",
    "  \"dbtable\": \"categories\",\n",
    "  \"user\": \"reviewsadmin\",\n",
    "  \"password\": \"Our_password@2023\",\n",
    "  \"require_secure_transport\": 'On',\n",
    "  \"useSSL\": True\n",
    "}\n",
    "\n",
    "con_busi_catego = {\n",
    "  \"host\": \"reviewsdatabase.mysql.database.azure.com\",\n",
    "  \"port\": 3306,\n",
    "  \"database\": \"reviews\",\n",
    "  \"dbtable\": \"bussiness_category\",\n",
    "  \"user\": \"reviewsadmin\",\n",
    "  \"password\": \"Our_password@2023\",\n",
    "  \"require_secure_transport\": 'On',\n",
    "  \"useSSL\": True\n",
    "}\n",
    "\n",
    "options4 = {\n",
    "  \"host\": \"reviewsdatabase.mysql.database.azure.com\",\n",
    "  \"port\": 3306,\n",
    "  \"database\": \"reviews\",\n",
    "  \"dbtable\": \"misc_bussin\",\n",
    "  \"user\": \"reviewsadmin\",\n",
    "  \"password\": \"Our_password@2023\",\n",
    "  \"require_secure_transport\": 'On',\n",
    "  \"useSSL\": True\n",
    "}\n",
    "\n",
    "#df_business = spark.read.format(\"mysql\").options(**options1).load()\n",
    "#df_categories = spark.read.format(\"mysql\").options(**options2).load()\n",
    "#df_bussiness_category = spark.read.format(\"mysql\").options(**options3).load()\n",
    "#df_misc_bussin = spark.read.format(\"mysql\").options(**options4).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32731463-ea58-4343-9aa9-30da7b0c4106",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def business_exist(gmap):\n",
    "    qsele = {\n",
    "        \"host\": \"reviewsdatabase.mysql.database.azure.com\",\n",
    "        \"port\": 3306,\n",
    "        \"database\": \"reviews\",\n",
    "        \"query\": f\"select gmap_id from business where gmap_id = '{gmap}'\",\n",
    "        \"user\": \"reviewsadmin\",\n",
    "        \"password\": \"Our_password@2023\",\n",
    "        \"require_secure_transport\": 'On',\n",
    "        \"useSSL\": True\n",
    "        }\n",
    "    df_sele = spark.read.format(\"mysql\").options(**qsele).load()\n",
    "    return df_sele.count() # si retorna 0 no encontró gmap_id en la tabla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6a82885-a0f3-4d83-99c8-7baeb2fa4f94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def catego_exist(cate):\n",
    "    qsele = {\n",
    "        \"host\": \"reviewsdatabase.mysql.database.azure.com\",\n",
    "        \"port\": 3306,\n",
    "        \"database\": \"reviews\",\n",
    "        \"query\": f\"select id_category from categories where name_category = '{cate}'\",\n",
    "        \"user\": \"reviewsadmin\",\n",
    "        \"password\": \"Our_password@2023\",\n",
    "        \"require_secure_transport\": 'On',\n",
    "        \"useSSL\": True\n",
    "        }\n",
    "    df_sele = spark.read.format(\"mysql\").options(**qsele).load()\n",
    "    return df_sele.count() # si retorna 0 no encontró la categoria en la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c46151f-6eff-4e84-b73f-a39dd3c1576b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def estado_retorna(lis):\n",
    "    # Dividir la cadena en partes separadas por comas\n",
    "    if lis == None:\n",
    "        return None\n",
    "        \n",
    "    partes = lis.split(',')\n",
    "\n",
    "    # Si hay al menos cuatro partes (tres comas), retornar las primeras 2 posiciones de la cuarta parte\n",
    "    if len(partes) >= 4:\n",
    "        cuarta_parte = partes[3]\n",
    "        estado_corto = cuarta_parte[:3]\n",
    "        return estado_corto.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d6971e-addd-458c-9beb-57182e84bd8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def inserta_categoria(categoria):\n",
    "    qsele = {\n",
    "    \"host\": \"reviewsdatabase.mysql.database.azure.com\",\n",
    "    \"port\": 3306,\n",
    "    \"database\": \"reviews\",\n",
    "    \"query\": \"select insert_category('categoria')\",\n",
    "    \"user\": \"reviewsadmin\",\n",
    "    \"password\": \"Our_password@2023\",\n",
    "    \"require_secure_transport\": 'On',\n",
    "    \"useSSL\": True\n",
    "        }\n",
    "    df_sele = spark.read.format(\"mysql\").options(**qsele).load()\n",
    "    valor_row = valor.select(\"insert_category('categoria')\").collect()[0]\n",
    "    valor_categoria = valor_row[\"insert_category('categoria')\"]\n",
    "    return valor_categoria    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b9bb10-77c9-4b3f-8b4c-0785644736ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def inserta_busi_catego(gmap_id,id_category):\n",
    "    df_sele = spark.createDataFrame([\n",
    "                (gmap_id, id_category)\n",
    "                ])\n",
    "   \n",
    "    df_sele.write.format(\"mysql\").options(**con_busi_catego).mode(\"append\").saveAsTable(\"business_category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2a07c1-5c16-4972-bcff-d48ce210db98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def inserta_business(gmap_id, name, address, latitude, longitude, url, state_corto):\n",
    "    ur = \"jdbc:mysql://reviewsdatabase.mysql.database.azure.com:3306/reviews?requireSSL=true\"\n",
    "    qsele = {\n",
    "        \"url\": ur,\n",
    "        \"dbtable\": \"business\",\n",
    "        \"user\": \"reviewsadmin\",\n",
    "        \"password\": \"Our_password@2023\",\n",
    "        \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "    }\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"gmap_id\", StringType(), nullable=False),\n",
    "        StructField(\"name\", StringType(), nullable=False),\n",
    "        StructField(\"address\", StringType(), nullable=True),\n",
    "        StructField(\"latitude\", StringType(), nullable=False),\n",
    "        StructField(\"longitude\", StringType(), nullable=False),\n",
    "        StructField(\"url\", StringType(), nullable=False),\n",
    "        StructField(\"state_corto\", StringType(), nullable=False)\n",
    "    ])\n",
    "    \n",
    "    data = [(gmap_id, name, address, float(latitude), float(longitude), url, state_corto)]\n",
    "    \n",
    "    # Crear el DataFrame utilizando el esquema y la sesión 'spark' de Databricks\n",
    "    df_insert = spark.createDataFrame(data, schema=schema)\n",
    "    \n",
    "    # Escribir en la base de datos usando el formato \"jdbc\"\n",
    "    df_insert.write.format(\"jdbc\").options(**qsele).mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5741cc6-97e0-44d8-b98a-e8e4119e61bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bussi_error = pd.DataFrame(columns=['gmap_id,name,address,latitude,longitude,relative_results,url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255f435e-86e2-4be8-8c70-b537f9aefb00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1447499638285740>:15\u001B[0m\n",
       "\u001B[1;32m     11\u001B[0m state_corto \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;66;03m#estado_retorna(address)\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m address:\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m#hay una falla en el dataset donde algunos name y address son iguales\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m#obtengo el nombre del business\u001B[39;00m\n",
       "\u001B[0;32m---> 15\u001B[0m     posicion_coma \u001B[38;5;241m=\u001B[39m name\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     16\u001B[0m     name \u001B[38;5;241m=\u001B[39m name[\u001B[38;5;241m0\u001B[39m:posicion_coma]\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m#obtengo la dirección    \u001B[39;00m\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: substring not found"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-1447499638285740>:15\u001B[0m\n\u001B[1;32m     11\u001B[0m state_corto \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;66;03m#estado_retorna(address)\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m address:\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m#hay una falla en el dataset donde algunos name y address son iguales\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m#obtengo el nombre del business\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     posicion_coma \u001B[38;5;241m=\u001B[39m name\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     16\u001B[0m     name \u001B[38;5;241m=\u001B[39m name[\u001B[38;5;241m0\u001B[39m:posicion_coma]\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m#obtengo la dirección    \u001B[39;00m\n\n\u001B[0;31mValueError\u001B[0m: substring not found",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: substring not found",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    cat_str = row[\"category\"]\n",
    "    gmap_id = row[\"gmap_id\"]\n",
    "    name = row[\"name\"]\n",
    "    address = row[\"address\"]\n",
    "    latitude = row[\"latitude\"]\n",
    "    longitude = row[\"longitude\"]\n",
    "    category_value = row[\"category\"]\n",
    "    relative_results = row[\"relative_results\"]\n",
    "    url = row[\"url\"]\n",
    "    state_corto = estado_retorna(address)\n",
    "    if address is None:\n",
    "        address = \"No se conoce\"\n",
    "        state_corto = 'XX'\n",
    "    \n",
    "    if name == address:\n",
    "        # Hay una falla en el dataset donde algunos name y address son iguales\n",
    "        # Obtengo el nombre del business\n",
    "        \n",
    "        if ',' in name:\n",
    "            posicion_coma = name.index(',')\n",
    "            name = name[0:posicion_coma]\n",
    "        else:\n",
    "            # Si no hay una coma en 'name', asegúrate de manejar este caso de manera adecuada\n",
    "            name = 'No se conoce el nombre'\n",
    "            state_corto = 'XX'\n",
    "\n",
    "        # Obtengo la dirección    \n",
    "        concurrencia = address.count(',')\n",
    "        if concurrencia > 0:\n",
    "            partes = address.split(',')\n",
    "            address = ','.join(partes[-3:])\n",
    "        else:\n",
    "            address = 'No se conoce la dirección'\n",
    "    \n",
    "    if len(name) > 250:\n",
    "            name = name[:250]\n",
    "    \n",
    "    if business_exist(gmap_id) == 0:\n",
    "        # compruebo si no encontro state_corto o no address estaba None\n",
    "        if state_corto is None:\n",
    "            state_corto = 'XX'\n",
    "\n",
    "        # Tabla Business: insertamos el registro\n",
    "        inserta_business(gmap_id, name, address, latitude, longitude, url, state_corto)\n",
    "        # Tabla Categoria\n",
    "        df_busi_cat = pd.DataFrame(columns=['gmap_id, categoria, id_category'])\n",
    "        if isinstance(category_value,(np.ndarray)):\n",
    "            for value in category_value:\n",
    "                valor = str(value[0])\n",
    "                new_row_cat = {\n",
    "                                \"gmap_id\": gmap_id,\n",
    "                                \"categoria\": valor\n",
    "                                }\n",
    "                rows_busi_cat.append(new_row_cat)\n",
    "            else:\n",
    "                new_row_cat = {\n",
    "                                \"gmap_id\": gmap_id,\n",
    "                                \"categoria\": valor\n",
    "                               }\n",
    "                rows_busi_cat.append(new_row_cat)\n",
    "        \n",
    "        for index, row in df_busi_cat.iterrows():\n",
    "            categoria = row['categoria']\n",
    "            gmap_id = row['gmap_id'] \n",
    "            if catego_exist(categoria) > 0:\n",
    "                continue\n",
    "            else:\n",
    "                valor = inserta_categoria(categoria)\n",
    "                row['id_category'] = valor\n",
    "                inserta_busi_catego(gmap_id,valor)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "metadata_sitios_unir_jsons",
   "widgets": {
    "input": {
     "currentValue": "",
     "nuid": "67ce05c6-0eb5-425b-bb02-a035093f1e2c",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "input",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "nombre_archivo": {
     "currentValue": "default.json",
     "nuid": "6ae63dfe-007c-4e83-a644-c8df71244b37",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default.json",
      "label": "Nombre del Archivo",
      "name": "nombre_archivo",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "param_name": {
     "currentValue": "",
     "nuid": "dac59532-364e-42d1-be08-81a0befaa46e",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "param_name",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
